Hadoop has several utilities to write MapReduce programs in python programming language:

They are:
	1. Hadoop Streaming
	2. Jython
	3. Dumbo
	4. Pydoop

Pydoop: Python + Hadoop
=======================
-> Pydoop is a package that provides a Python API for Hadoop HDFS and Hadoop MapReduce.
-> Pydoop is developed and maintained by researchers at CRS4 – Distributed Computing group.
-> Pydoop is a Python wrapper of the C++ one
-> It runs on Hadoop Pipes architecture [pipes is a C++ utitlity to write MapReduces in C++]
-> YARN is now fully supported
-> Added support for CDH 4.4.0 and CDH 4.5.0
-> Added support for hadoop 2.2.0
-> Current version: 0.12.0
-> It has several advantages over Hadoop’s built-in solutions for Python programming (Streaming and Jython)
-> It is CPython package
-> It allows several other third party python modules like scipy, numpy, pandas, anaconda, nltk, etc...
-> We can add custome counters using Pydoop.

	def mapper(_, text, writer):
  	    wordlist = text.split()
  	    for word in wordlist:
    	        writer.emit(word, "1")
                writer.count("num words", len(wordlist))

-> We can perform several python string, math, other functions.
-> We can write custom record readers, record writers, combiners, partitioners etc...

-> Pydoop API is object oriented, developer writes Mapper class to perform map function, Reducer class to perform reduce function.

	#!/usr/bin/env python
	import pydoop.pipes as pp

	class Mapper(pp.Mapper):

  	    def map(self, context):
    	        words = context.getInputValue().split()
                for w in words:
                    context.emit(w, "1")

	class Reducer(pp.Reducer):

            def reduce(self, context):
                s = 0
                while context.nextValue():
                    s += int(context.getInputValue())
                    context.emit(context.getInputKey(), str(s))

if __name__ == "__main__":
    pp.runTask(pp.Factory(Mapper, Reducer))

Usage: 
------
hadoop fs -put test.py /pydoop
hadoop pipes -D hadoop.pipes.java.recordreader=true  -D hadoop.pipes.java.recordwriter=true -program /pydoop/test.py 
-input /pydoop/hdfs -output wordc

-> HDFS API: Pydoop allows us to connect to an HDFS installation, read and write files and get information on files, 
directories and global file system properties


Pydoop Script:
--------------
Pydoop Script is the easiest way to write simple MapReduce programs for Hadoop. With Pydoop Script, your code focuses on the core of the MapReduce model: the mapper and reducer functions

Pydoop applications are run as Hadoop Pipes applications.

Usage: pydoop script [Options] scriptname.py <input> <output> [Hadoop Generic Options]

Options are:

Short	Long			Meaning
-m	--map-fn		Name of map function within module (default: mapper)
-r	--reduce-fn		Name of reduce function within module (default: reducer)
-c	--combine-fn		Name of combine function within module (default: None)
-t	--kv-separator		Key-value separator string in final output (default: <tab> character)
 	--num-reducers		Number of reduce tasks. Specify 0 to only perform map phase (default: 3 * num task trackers)
-D	 			Set a property value, such as -D mapred.compress.map.output=true

Hadoop Generic Options:

-conf <configuration file>	specify an application configuration file
-fs <local|namenode:port>	specify a namenode
-jt <local|jobtracker:port>	specify a job tracker
-files <list of files>		comma-separated files to be copied to the map reduce cluster
-libjars <list of jars>		comma-separated jar files to include in the classpath
-archives <list of archives>	comma-separated archives to be unarchived on the compute machines


Hadoop SequenceFile Format:
---------------------------

Using Sequence Files within Pydoop is easy. To write output key/value pairs in the SequenceFile format, add the following properties to your job configuration file:

mapred.output.format.class = org.apache.hadoop.mapred.SequenceFileOutputFormat
mapred.output.compression.type = desired_compression_type [NONE|RECORD|BLOCK]

Compression type can be NONE, RECORD or BLOCK. 

To take the SequenceFile as input format, we can use the following parameter:

mapred.input.format.class = org.apache.hadoop.mapred.SequenceFileInputFormat

******************************************************************************************************************

Practice:.
---------
Script1: wordcount.py
---------------------

# Compute the word frequency

import pydoop

def mapper(_, text, writer):
    for word in text.split():
        writer.emit(word, "1")

def reducer(word, icounts, writer):
    writer.emit(word, sum(map(int, icounts)))

def combiner(word, icounts, writer):
    writer.count('combiner calls', 1)
    reducer(word, icounts, writer)

Usage: pydoop script -c combiner wordcount.py /pydoop/hdfs /pydoop/wordcount

Note: we need to explicitly set the -c flag to activate the combiner. By default, no combiner is called

======================================================================
Dataset: Stocks:
----------------
It has 9 columns, they are:

	1. market
	2. stockcode
	3. date
	4. open value
	5. high value
	6. low value
	7. close value
	8. volume
	9. previous close

Script2: volumesum.py
----------------------
# This Script is used to compute the total volume of every stock

def mapper(_, text, writer):
    columns = text.split("\t")
    writer.emit(columns[1], int(columns[7]));

def reducer(key,values,writer):
    writer.emit(key, sum(map(int,values)))

Usage: pydoop script volumesum.py /pydoop/stocks /pydoop/sumvolume

Script3: volumemin.py
---------------------
# This Script is used to compute the minimum volume of every stock

def mapper(_, text, writer):
    columns = text.split("\t")
    writer.emit(columns[1], int(columns[7]));

def reducer(key,values,writer):
    data = map(int,values)
    volmin = min(data)
    writer.emit(key, volmin)

Usage: pydoop script volumemin.py /pydoop/stocks /pydoop/minvolume 

Script4: volumemax.py
---------------------
# This Script is used to compute the maximum volume of every stock

def mapper(_, text, writer):
    columns = text.split("\t")
    writer.emit(columns[1], int(columns[7]));

def reducer(key,values,writer):
    writer.emit(key, max(map(int,values)))

Usage: pydoop script volumemax.py /pydoop/stocks /pydoop/maxvolume 

=======================================================================================



















Appendix:
---------
pydoop.pipes 	—> 	MapReduce API
	Classes Instantiated by the Framework

pydoop.hdfs 	—> HDFS API
	Configuration
	pydoop.hdfs.path – Path Name Manipulations
	pydoop.hdfs.fs – File System Handles
	pydoop.hdfs.file – HDFS File Objects

pydoop.utils 	—> Utility Functions
pydoop.jc 	—> Pydoop Script Configuration Access
pydoop.hadut 	–> Run Hadoop Shell Commands



Pydoop is not the only way to write Hadoop applications in Python. The “built-in” solutions are Hadoop Streaming with Python executables and Jython. The main disadvantages with these approaches are the following:

Streaming does not provide an API: the developer writes a mapper and a reducer script that communicate with the framework via standard input/output. The programming style is therefore rather awkward, especially in the case of reducers, where developers must manually handle key switching. More importantly, there is no way to write a Python RecordReader, RecordWriter or Partitioner. In Hadoop versions that do not include the HADOOP-1722 patch, Streaming has the additional limitation of only being able to process UTF-8 text records;
Jython is a Java implementation of the Python language: the standard C implementation, in cases where ambiguity may arise, is referred to as “CPython”. Although this approach gives access to the full Hadoop API, it is limited by the fact that CPython modules (including part of the standard library) cannot be used.
As a consequence, few Python programmers use Streaming or Jython directly. A popular alternative is offered by Dumbo, a programming framework built as a wrapper around Streaming that allows for a more elegant coding style and also includes facilities to make job running easier. Its author, Klaas Bosteels, is also the author of the aforementioned patch for Streaming.

However, writing Python components other than the mapper, reducer and combiner is not possible in Dumbo, and there is no HDFS API. Pydoop, on the other hand, gives you almost complete access to MapReduce components (you can write a Python RecordReader, RecordWriter and Partitioner) and to HDFS without adding much complexity. As an example, in this section we will show how to rewrite Dumbo’s tutorial example in Pydoop. Throughout the rest of this section we refer to examples and results from Dumbo version 0.21.28.

=======================================================================================
References:
-----------
http://pydoop.sourceforge.net/docs/index.html
http://dl.acm.org/citation.cfm?doid=1851476.1851594
http://hadoopecosystem.blogspot.in/2014/08/hadoop-streaming-utility-from-hadoop.html?spref=bl
http://it-ebooks.info/book/1041/




Others:
-------
CPython is the default, most-widely used implementation of the Python programming language. It is written in C. In addition to CPython, there are other "production-quality" Python implementations: Jython, written in Java, PyPy, and IronPython, which is written for the Common Language Infrastructure. There are also several experimental implementations.[1]
